#!/usr/bin/env python3
"""
Pinecone Agent Web Interface
Flask-based web UI for Pinecone vector database operations with RAG support.
"""

# SSL Certificate configuration (must be done before importing httpx/urllib3)
import os
import certifi
os.environ['SSL_CERT_FILE'] = certifi.where()
os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()

import re
import json
import logging
import unicodedata
import httpx
from pathlib import Path
from flask import Flask, render_template, request, jsonify, Response, stream_with_context, send_from_directory
from dotenv import load_dotenv
from openai import OpenAI

# Import calculator modules
from calculator import WageCalculator, InsuranceCalculator, CompanySize, IndustryType

# Import RAG enhancement modules
from src.query_enhancer import QueryEnhancer
from src.context_optimizer import ContextOptimizer
from src.reranker import get_reranker
from src.hybrid_searcher import get_hybrid_searcher
from msds_client import MsdsApiClient

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = os.urandom(24)

# Documents folder path for serving images
DOCUMENTS_PATH = Path(__file__).parent / "documents"

# Domain-specific system prompts
DOMAIN_PROMPTS = {
    "laborlaw": """ë‹¹ì‹ ì€ í•œêµ­ ë…¸ë™ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë²•ë¥  ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ê³„ì‚°ê¸° ë„êµ¬ ì‚¬ìš© (ì¤‘ìš”)

**ì„ê¸ˆ ë° ë³´í—˜ë£Œ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ë°˜ë“œì‹œ ì œê³µëœ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:**

1. **calculate_wage**: ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì†Œë“ì„¸ ê³„ì‚°
   - ì‚¬ìš© ì‹œê¸°: ì—°ë´‰/ì›”ê¸‰ì—ì„œ ì‹¤ì œ ë°›ëŠ” ê¸‰ì—¬ë¥¼ ê³„ì‚°í•  ë•Œ
   - ì˜ˆì‹œ: "ì—°ë´‰ 5000ë§Œì› ì‹¤ìˆ˜ë ¹ì•¡ì€?", "ì›”ê¸‰ 300ë§Œì› ì„¸ê¸ˆ ì–¼ë§ˆ?"

2. **calculate_insurance**: 4ëŒ€ë³´í—˜ë£Œ ìƒì„¸ ê³„ì‚°
   - ì‚¬ìš© ì‹œê¸°: ì—…ì¢…ë³„/ê·œëª¨ë³„ ë³´í—˜ë£Œë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ì„ ë•Œ
   - ì˜ˆì‹œ: "ê±´ì„¤ì—… 4ëŒ€ë³´í—˜ë£ŒëŠ”?", "300ë§Œì› êµ­ë¯¼ì—°ê¸ˆ ì–¼ë§ˆ?"

**í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ í›„ì—ëŠ” ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì •ë¦¬í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì œì‹œí•˜ì„¸ìš”.**

## í•„ìˆ˜ ê³„ì‚° ê·œì¹™ (ìˆ˜ë™ ê³„ì‚° ì‹œ)

### ì£¼íœ´ìˆ˜ë‹¹
- ì¡°ê±´: ì£¼ 15ì‹œê°„ ì´ìƒ ê·¼ë¬´ ì‹œ ì˜ë¬´ ì§€ê¸‰
- ê³µì‹: ì£¼íœ´ìˆ˜ë‹¹ = (ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ Ã· 40) Ã— 8 Ã— ì‹œê¸‰
- **ì£¼ê¸‰/ì›”ê¸‰ ê³„ì‚° ì‹œ ë°˜ë“œì‹œ ì£¼íœ´ìˆ˜ë‹¹ì„ í¬í•¨í•˜ì—¬ ë‹µë³€**

### ì›”ê¸‰ ê³„ì‚°
- ì›” í™˜ì‚°: ì£¼ë‹¹ì‹œê°„ Ã— 4.345ì£¼
- ì£¼íœ´ í¬í•¨ ì›”ê¸‰: (ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ + ì£¼íœ´ì‹œê°„) Ã— 4.345 Ã— ì‹œê¸‰

### ê°€ì‚°ìˆ˜ë‹¹
- ì—°ì¥ê·¼ë¡œ(8ì‹œê°„ ì´ˆê³¼): í†µìƒì„ê¸ˆ Ã— 1.5ë°°
- ì•¼ê°„ê·¼ë¡œ(22ì‹œ~06ì‹œ): í†µìƒì„ê¸ˆ Ã— 1.5ë°°
- íœ´ì¼ê·¼ë¡œ: í†µìƒì„ê¸ˆ Ã— 1.5ë°° (8ì‹œê°„ ì´ˆê³¼ ì‹œ 2.0ë°°)

## ë‹µë³€ ì§€ì¹¨
1. **ì„ê¸ˆ/ë³´í—˜ë£Œ ê³„ì‚° ì‹œ í•¨ìˆ˜ë¥¼ ìš°ì„  ì‚¬ìš©**í•˜ê³ , ê²°ê³¼ë¥¼ í‘œë¡œ ì •ë¦¬
2. ê³„ì‚° ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
3. ê´€ë ¨ ë²•ì¡°í•­ ì¸ìš© ì‹œ [1], [2] í˜•ì‹ ì‚¬ìš©
4. ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ê³„ì‚° ë‚´ì—­ ì •ë¦¬
5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”""",
}

# Default system prompt (semiconductor/general domain)
DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **ì¤‘ìš”**: ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. ì˜ˆ: "CVD ê³µì •ì€ í™”í•™ ê¸°ìƒ ì¦ì°© ë°©ì‹ì…ë‹ˆë‹¤[1]."
3. ì¸ìš© í˜•ì‹: ë¬¸ì¥ ëì— [1], [2] ë“±ì˜ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì–´ë–¤ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
4. ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•  ë•ŒëŠ” [1][3]ì²˜ëŸ¼ ë³µìˆ˜ ì¸ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤
5. ê¸°ìˆ  ìš©ì–´ëŠ” í•œê¸€ê³¼ ì˜ë¬¸ì„ ë³‘ê¸°í•˜ì„¸ìš”
6. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì„¸ìš”
7. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
8. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”
9. **ì¤‘ìš”**: ì´ë¯¸ì§€ëŠ” ì ˆëŒ€ ì§ì ‘ ì‚½ì…í•˜ì§€ ë§ˆì„¸ìš” (![image](...) í˜•ì‹ ì‚¬ìš© ê¸ˆì§€). ê´€ë ¨ ì´ë¯¸ì§€ëŠ” ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""

# Global instances (lazy initialization)
_agent = None
_openai_client = None
_query_enhancer = None
_context_optimizer = None
_reranker = None
_hybrid_searcher = None

def get_openai_client():
    """Get or create OpenAI client with SSL certificate configuration."""
    global _openai_client
    if _openai_client is None:
        # Create httpx client with explicit SSL certificate verification
        http_client = httpx.Client(verify=certifi.where())
        _openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            http_client=http_client
        )
    return _openai_client

def get_agent():
    """Get or create the PineconeAgent instance."""
    global _agent
    if _agent is None:
        from src.agent import PineconeAgent
        _agent = PineconeAgent(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            pinecone_api_key=os.getenv("PINECONE_API_KEY"),
            pinecone_index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
            create_index_if_not_exists=False
        )
    return _agent


def get_query_enhancer():
    """Get or create QueryEnhancer instance."""
    global _query_enhancer
    if _query_enhancer is None:
        _query_enhancer = QueryEnhancer(os.getenv("OPENAI_API_KEY"))
    return _query_enhancer


def get_context_optimizer():
    """Get or create ContextOptimizer instance."""
    global _context_optimizer
    if _context_optimizer is None:
        _context_optimizer = ContextOptimizer(os.getenv("OPENAI_API_KEY"))
    return _context_optimizer


def get_pinecone_client():
    """Get Pinecone client from agent or create standalone."""
    try:
        agent = get_agent()
        return agent.pinecone_uploader.pc
    except Exception:
        try:
            from pinecone import Pinecone
            return Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        except Exception:
            return None


def get_reranker_instance():
    """Get or create Reranker instance (prefers Pinecone Inference API)."""
    global _reranker
    if _reranker is None:
        pc = get_pinecone_client()
        _reranker = get_reranker(use_cross_encoder=True, pinecone_client=pc)
    return _reranker


def get_hybrid_searcher_instance():
    """Get or create HybridSearcher instance."""
    global _hybrid_searcher
    if _hybrid_searcher is None:
        _hybrid_searcher = get_hybrid_searcher()
    return _hybrid_searcher

def get_uploader():
    """Get PineconeUploader for stats."""
    from src.pinecone_uploader import PineconeUploader
    return PineconeUploader(
        api_key=os.getenv("PINECONE_API_KEY"),
        index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
        create_if_not_exists=False
    )


# ============================================
# Calculator Functions for GPT Function Calling
# ============================================

def calculate_wage(
    salary_type: str,
    amount: int,
    tax_free_monthly: int = 0,
    dependents: int = 1,
    children_8_to_20: int = 0,
    company_size: str = 'small'
) -> dict:
    """
    ì„ê¸ˆ ê³„ì‚° (ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì„¸ê¸ˆ)

    Args:
        salary_type: 'ì—°ë´‰' ë˜ëŠ” 'ì›”ê¸‰'
        amount: ê¸‰ì—¬ì•¡ (ì›)
        tax_free_monthly: ì›” ë¹„ê³¼ì„¸ì•¡ (ìµœëŒ€ 200,000ì›)
        dependents: ë¶€ì–‘ê°€ì¡± ìˆ˜ (ë³¸ì¸ í¬í•¨, ê¸°ë³¸ 1ëª…)
        children_8_to_20: 8~20ì„¸ ìë…€ ìˆ˜ (ê¸°ë³¸ 0ëª…)
        company_size: íšŒì‚¬ ê·œëª¨ ('small': 150ì¸ ë¯¸ë§Œ, 'medium': 150~999ì¸, 'large': 1000ì¸ ì´ìƒ)

    Returns:
        dict: ê³„ì‚° ê²°ê³¼ (ì…ë ¥ì •ë³´, ê·¼ë¡œìê³µì œë‚´ì—­, íšŒì‚¬ë¶€ë‹´ë‚´ì—­, ì‹¤ìˆ˜ë ¹ì•¡)
    """
    calc = WageCalculator()

    if salary_type == 'ì—°ë´‰':
        result = calc.calculate_from_annual(
            annual_salary=amount,
            tax_free_monthly=tax_free_monthly,
            dependents=dependents,
            children_8_to_20=children_8_to_20,
            company_size=company_size
        )
    else:  # ì›”ê¸‰
        result = calc.calculate_from_monthly(
            monthly_salary=amount,
            tax_free_monthly=tax_free_monthly,
            dependents=dependents,
            children_8_to_20=children_8_to_20,
            company_size=company_size
        )

    return result


def calculate_insurance(
    monthly_income: int,
    non_taxable: int = 0,
    company_size_code: str = 'UNDER_150',
    industry_code: str = 'OTHERS'
) -> dict:
    """
    4ëŒ€ë³´í—˜ë£Œ ê³„ì‚° (êµ­ë¯¼ì—°ê¸ˆ, ê±´ê°•ë³´í—˜, ì¥ê¸°ìš”ì–‘ë³´í—˜, ê³ ìš©ë³´í—˜, ì‚°ì¬ë³´í—˜)

    Args:
        monthly_income: ì›” ì†Œë“ (ì›)
        non_taxable: ë¹„ê³¼ì„¸ì†Œë“ (ì›)
        company_size_code: íšŒì‚¬ ê·œëª¨ ì½”ë“œ
            - 'UNDER_150': 150ì¸ ë¯¸ë§Œ
            - 'PRIORITY_SUPPORT': 150ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…
            - 'FROM_150_TO_999': 150~999ì¸
            - 'OVER_1000': 1000ì¸ ì´ìƒ
        industry_code: ì—…ì¢… ì½”ë“œ (ì‚°ì¬ë³´í—˜ ìš”ìœ¨)
            - 'OTHERS': ê¸°íƒ€ì‚¬ì—… (0.9%)
            - 'WHOLESALE_RETAIL': ë„ì†Œë§¤/ìŒì‹/ìˆ™ë°• (0.7%)
            - 'PROFESSIONAL': ì „ë¬¸/ê³¼í•™/ê¸°ìˆ  (0.5%)
            - 'FINANCE_INSURANCE': ê¸ˆìœµ/ë³´í—˜ (0.5%)
            - 'CONSTRUCTION': ê±´ì„¤ì—… (3.5%)
            - ê¸°íƒ€: IndustryType enum ì°¸ê³ 

    Returns:
        dict: ë³´í—˜ë£Œ ìƒì„¸ ë‚´ì—­ (ì…ë ¥ì •ë³´, ê° ë³´í—˜ë³„ ë‚´ì—­, í•©ê³„)
    """
    calc = InsuranceCalculator()

    # Convert string codes to enums
    size_map = {
        'UNDER_150': CompanySize.UNDER_150,
        'PRIORITY_SUPPORT': CompanySize.PRIORITY_SUPPORT,
        'FROM_150_TO_999': CompanySize.FROM_150_TO_999,
        'OVER_1000': CompanySize.OVER_1000
    }
    company_size = size_map.get(company_size_code, CompanySize.UNDER_150)

    # Find industry type
    industry = IndustryType.OTHERS
    for ind in IndustryType:
        if ind.name == industry_code:
            industry = ind
            break

    result = calc.calculate_all(
        monthly_income=monthly_income,
        non_taxable=non_taxable,
        company_size=company_size,
        industry=industry
    )

    return result


# GPT Function definitions for Function Calling
CALCULATOR_FUNCTIONS = [
    {
        "type": "function",
        "function": {
            "name": "calculate_wage",
            "description": "í•œêµ­ ë…¸ë™ë²•ì— ë”°ë¥¸ ì„ê¸ˆ ê³„ì‚° (ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì†Œë“ì„¸, ì§€ë°©ì†Œë“ì„¸). ì—°ë´‰ì´ë‚˜ ì›”ê¸‰ì„ ì…ë ¥í•˜ë©´ ì‹¤ì œ ë°›ê²Œ ë˜ëŠ” ê¸‰ì—¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "salary_type": {
                        "type": "string",
                        "enum": ["ì—°ë´‰", "ì›”ê¸‰"],
                        "description": "ê¸‰ì—¬ ìœ í˜•"
                    },
                    "amount": {
                        "type": "integer",
                        "description": "ê¸‰ì—¬ì•¡ (ì› ë‹¨ìœ„)"
                    },
                    "tax_free_monthly": {
                        "type": "integer",
                        "description": "ì›” ë¹„ê³¼ì„¸ì•¡ (ì‹ëŒ€ ë“±, ìµœëŒ€ 200,000ì›)",
                        "default": 0
                    },
                    "dependents": {
                        "type": "integer",
                        "description": "ë¶€ì–‘ê°€ì¡± ìˆ˜ (ë³¸ì¸ í¬í•¨)",
                        "default": 1
                    },
                    "children_8_to_20": {
                        "type": "integer",
                        "description": "8~20ì„¸ ìë…€ ìˆ˜ (ìë…€ì„¸ì•¡ê³µì œ ëŒ€ìƒ)",
                        "default": 0
                    },
                    "company_size": {
                        "type": "string",
                        "enum": ["small", "medium", "large"],
                        "description": "íšŒì‚¬ ê·œëª¨: small(150ì¸ ë¯¸ë§Œ), medium(150~999ì¸), large(1000ì¸ ì´ìƒ)",
                        "default": "small"
                    }
                },
                "required": ["salary_type", "amount"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate_insurance",
            "description": "í•œêµ­ 4ëŒ€ë³´í—˜ë£Œ ìƒì„¸ ê³„ì‚° (êµ­ë¯¼ì—°ê¸ˆ, ê±´ê°•ë³´í—˜, ì¥ê¸°ìš”ì–‘ë³´í—˜, ê³ ìš©ë³´í—˜, ì‚°ì¬ë³´í—˜). ê·¼ë¡œìì™€ ì‚¬ì—…ì£¼ê°€ ê°ê° ë¶€ë‹´í•˜ëŠ” ë³´í—˜ë£Œë¥¼ ì—…ì¢…ê³¼ íšŒì‚¬ ê·œëª¨ì— ë”°ë¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "monthly_income": {
                        "type": "integer",
                        "description": "ì›” ì†Œë“ (ì› ë‹¨ìœ„)"
                    },
                    "non_taxable": {
                        "type": "integer",
                        "description": "ë¹„ê³¼ì„¸ì†Œë“ (ì› ë‹¨ìœ„)",
                        "default": 0
                    },
                    "company_size_code": {
                        "type": "string",
                        "enum": ["UNDER_150", "PRIORITY_SUPPORT", "FROM_150_TO_999", "OVER_1000"],
                        "description": "íšŒì‚¬ ê·œëª¨: UNDER_150(150ì¸ ë¯¸ë§Œ), PRIORITY_SUPPORT(ìš°ì„ ì§€ì›ëŒ€ìƒ), FROM_150_TO_999(150~999ì¸), OVER_1000(1000ì¸ ì´ìƒ)",
                        "default": "UNDER_150"
                    },
                    "industry_code": {
                        "type": "string",
                        "description": "ì‚°ì¬ë³´í—˜ ì—…ì¢… ì½”ë“œ (ì˜ˆ: OTHERS-ê¸°íƒ€ì‚¬ì—…, WHOLESALE_RETAIL-ë„ì†Œë§¤, PROFESSIONAL-ì „ë¬¸ì„œë¹„ìŠ¤, FINANCE_INSURANCE-ê¸ˆìœµ, CONSTRUCTION-ê±´ì„¤)",
                        "default": "OTHERS"
                    }
                },
                "required": ["monthly_income"]
            }
        }
    }
]


def _build_ncs_filter(query: str) -> dict:
    """Build Pinecone metadata filter from NCS-related query patterns."""
    import re
    filters = {}

    # Detect NCS category mentions
    ncs_categories = {
        'ë°˜ë„ì²´ê°œë°œ': ['ë°˜ë„ì²´ ê°œë°œ', 'ë°˜ë„ì²´ê°œë°œ', 'ì œí’ˆ ê¸°íš', 'ì•„í‚¤í…ì²˜', 'íšŒë¡œ ì„¤ê³„'],
        'ë°˜ë„ì²´ì¥ë¹„': ['ë°˜ë„ì²´ ì¥ë¹„', 'ë°˜ë„ì²´ì¥ë¹„', 'ì¥ë¹„'],
        'ë°˜ë„ì²´ì¬ë£Œ': ['ë°˜ë„ì²´ ì¬ë£Œ', 'ë°˜ë„ì²´ì¬ë£Œ', 'ì¬ë£Œ'],
        'ë°˜ë„ì²´ì œì¡°': ['ë°˜ë„ì²´ ì œì¡°', 'ë°˜ë„ì²´ì œì¡°', 'ì œì¡° ê³µì •'],
    }
    for cat, keywords in ncs_categories.items():
        for kw in keywords:
            if kw in query:
                filters['ncs_category'] = cat
                break
        if 'ncs_category' in filters:
            break

    # Detect NCS section type from query
    section_patterns = {
        'required_knowledge': ['í•„ìš” ì§€ì‹', 'í•„ìš”ì§€ì‹', 'ì´ë¡ ì  ë°°ê²½', 'ê°œë… ì„¤ëª…'],
        'performance_procedure': ['ìˆ˜í–‰ ìˆœì„œ', 'ìˆ˜í–‰ìˆœì„œ', 'ì‘ì—… ì ˆì°¨', 'ì‹¤ìŠµ ì ˆì°¨'],
        'performance_content': ['ìˆ˜í–‰ ë‚´ìš©', 'ìˆ˜í–‰ë‚´ìš©'],
        'performance_tip': ['ìˆ˜í–‰ tip', 'ìˆ˜í–‰ íŒ', 'ì‹¤ë¬´ íŒ'],
        'learning_objective': ['í•™ìŠµ ëª©í‘œ', 'í•™ìŠµëª©í‘œ'],
        'evaluation_criteria': ['í‰ê°€ ì¤€ê±°', 'í‰ê°€ì¤€ê±°', 'í‰ê°€ ê¸°ì¤€'],
        'evaluation_method': ['í‰ê°€ ë°©ë²•', 'í‰ê°€ë°©ë²•'],
        'feedback': ['í”¼ë“œë°±'],
        'teaching_method': ['êµìˆ˜ ë°©ë²•', 'êµìˆ˜ë°©ë²•', 'ê°€ë¥´ì¹˜ëŠ” ë°©ë²•'],
        'learning_method': ['í•™ìŠµ ë°©ë²•', 'í•™ìŠµë°©ë²•', 'ê³µë¶€ ë°©ë²•'],
        'safety_notes': ['ì•ˆì „', 'ìœ ì˜ì‚¬í•­', 'ì£¼ì˜ì‚¬í•­'],
        'key_terms': ['í•µì‹¬ ìš©ì–´', 'ìš©ì–´ ì •ë¦¬', 'í•µì‹¬ìš©ì–´'],
        'prerequisite': ['ì„ ìˆ˜í•™ìŠµ', 'ì„ ìˆ˜ í•™ìŠµ', 'ì‚¬ì „ ì§€ì‹'],
        'equipment': ['ì¥ë¹„', 'ê³µêµ¬', 'ê¸°ê¸°'],
    }
    for section_type, keywords in section_patterns.items():
        for kw in keywords:
            if kw in query:
                filters['ncs_section_type'] = section_type
                break
        if 'ncs_section_type' in filters:
            break

    # Detect learning unit mention
    lu_match = re.search(r'í•™ìŠµ\s*(\d+)', query)
    if lu_match:
        filters['learning_unit'] = int(lu_match.group(1))

    return filters if filters else None


def parse_mentions(query):
    """Parse @mentions from query and return (clean_query, filters).

    Supports:
    - @íŒŒì¼ëª….md - specific file
    - @í´ë”ëª…/ - folder path (ends with /)
    - @í‚¤ì›Œë“œ - partial match on source_file
    """
    mentions = re.findall(r'@([^\s@]+)', query)
    clean_query = re.sub(r'@[^\s@]+', '', query).strip()

    filters = []
    for mention in mentions:
        if mention.endswith('/'):
            # Folder filter
            filters.append({'type': 'folder', 'value': mention.rstrip('/')})
        elif '.' in mention:
            # File filter (has extension)
            filters.append({'type': 'file', 'value': mention})
        else:
            # Keyword filter
            filters.append({'type': 'keyword', 'value': mention})

    return clean_query, filters


def build_source_filter(filters):
    """Build Pinecone filter from parsed mentions.

    Note: Pinecone doesn't support substring matching directly,
    so we'll filter results post-query.
    """
    # For now, return None and do post-filtering
    # Pinecone filter would require exact match or $in operator
    return None


# Domain configurations
DOMAIN_CONFIG = {
    'semiconductor': {
        'title': 'ë°˜ë„ì²´ AI ê²€ìƒ‰',
        'icon': 'ğŸ”¬',
        'namespace': '',  # default namespace
        'color': '#00d4ff',
        'color_rgb': '0, 212, 255',
        'gradient_from': '#00d4ff',
        'gradient_to': '#0099cc',
        'description': 'ë°˜ë„ì²´ ê¸°ìˆ  ë¬¸ì„œ ê¸°ë°˜ AI ì§ˆì˜ì‘ë‹µ',
        'sample_questions': [
            'CVDì™€ PVD ê³µì •ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?',
            'ë°˜ë„ì²´ íŒ¨í‚¤ì§• ê³µì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”',
            'ì›¨ì´í¼ ì œì¡° ê³¼ì •ì˜ ì£¼ìš” ë‹¨ê³„ëŠ”?'
        ],
        'features': ['ê³µì • ê¸°ìˆ ', 'ì•„í‚¤í…ì²˜', 'ì œì¡° ê³µì •', 'í’ˆì§ˆ ê´€ë¦¬']
    },
    'laborlaw': {
        'title': 'ë…¸ë™ë²• AI ìƒë‹´',
        'icon': 'âš–ï¸',
        'namespace': 'laborlaw',
        'color': '#ff9800',
        'color_rgb': '255, 152, 0',
        'gradient_from': '#ff9800',
        'gradient_to': '#f57c00',
        'description': 'ë…¸ë™ë²• ë° ì„ê¸ˆÂ·ë³´í—˜ë£Œ ê³„ì‚° ì „ë¬¸ AI',
        'sample_questions': [
            'ì—°ë´‰ 5000ë§Œì› ì‹¤ìˆ˜ë ¹ì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?',
            'ì£¼íœ´ìˆ˜ë‹¹ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”',
            'ë¶€ë‹¹í•´ê³  íŒë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?',
            '4ëŒ€ë³´í—˜ë£ŒëŠ” ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?'
        ],
        'features': ['ì„ê¸ˆ ê³„ì‚°', '4ëŒ€ë³´í—˜', 'ê·¼ë¡œê¸°ì¤€ë²•', 'íŒë¡€ ê²€ìƒ‰', 'ë²•ë¥  ìƒë‹´']
    },
    'msds': {
        'title': 'MSDS í™”í•™ë¬¼ì§ˆ ì •ë³´',
        'icon': 'ğŸ§ª',
        'namespace': 'msds',
        'color': '#4caf50',
        'color_rgb': '76, 175, 80',
        'gradient_from': '#4caf50',
        'gradient_to': '#388e3c',
        'description': 'ì‚°ì—…ì•ˆì „ë³´ê±´ê³µë‹¨ ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ ê²€ìƒ‰',
        'sample_questions': [
            'ë²¤ì  ì˜ ì•ˆì „ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”',
            'ì—íƒ„ì˜¬ì˜ ìœ í•´ì„±Â·ìœ„í—˜ì„±ì€?',
            'í†¨ë£¨ì—”ì˜ ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹ì„ ì°¾ì•„ì¤˜',
            'CAS ë²ˆí˜¸ë¡œ í™”í•™ë¬¼ì§ˆ ê²€ìƒ‰'
        ],
        'features': ['í™”í•™ë¬¼ì§ˆ ê²€ìƒ‰', 'MSDS ì •ë³´', 'ì•ˆì „ ë°ì´í„°', 'CAS ë²ˆí˜¸ ì¡°íšŒ']
    }
}


@app.route('/')
def home():
    """Home page with domain selection."""
    return render_template('home.html')


@app.route('/semiconductor')
def semiconductor():
    """Semiconductor domain page."""
    config = DOMAIN_CONFIG['semiconductor']
    return render_template('domain.html', domain='semiconductor', config=config)


@app.route('/laborlaw')
def laborlaw():
    """Labor law domain page."""
    config = DOMAIN_CONFIG['laborlaw']
    return render_template('domain.html', domain='laborlaw', config=config)


@app.route('/msds')
def msds():
    """MSDS chemical information page."""
    config = DOMAIN_CONFIG['msds']
    return render_template('msds.html', domain='msds', config=config)


@app.route('/documents/<path:filepath>')
def serve_document(filepath):
    """Serve document images and files."""
    # Normalize unicode for macOS (NFD filesystem) compatibility
    filepath = unicodedata.normalize('NFC', filepath)
    # Try NFC first, fall back to NFD if not found
    full_path = DOCUMENTS_PATH / filepath
    if not full_path.exists():
        filepath_nfd = unicodedata.normalize('NFD', filepath)
        full_path_nfd = DOCUMENTS_PATH / filepath_nfd
        if full_path_nfd.exists():
            filepath = filepath_nfd
    return send_from_directory(DOCUMENTS_PATH, filepath)


def find_related_images(source_file: str) -> list:
    """Find related images from the same document folder."""
    images = []
    try:
        # Normalize unicode for consistent path matching
        normalized_file = unicodedata.normalize('NFC', source_file)
        # Extract the folder path from source_file
        # e.g., "ncs/ë°˜ë„ì²´ê°œë°œ/LM1903060102_23v5_ë°˜ë„ì²´_ì•„í‚¤í…ì²˜_ì„¤ê³„/..." -> folder path
        parts = normalized_file.split('/')
        if len(parts) >= 3:
            folder_path = DOCUMENTS_PATH / parts[0] / parts[1] / parts[2]
            # Try NFD form if NFC path doesn't exist (macOS filesystem)
            if not folder_path.exists():
                nfd_file = unicodedata.normalize('NFD', source_file)
                nfd_parts = nfd_file.split('/')
                folder_path = DOCUMENTS_PATH / nfd_parts[0] / nfd_parts[1] / nfd_parts[2]
            if folder_path.exists():
                # Find all image files in the folder
                for ext in ['*.jpeg', '*.jpg', '*.png']:
                    for img_file in folder_path.glob(ext):
                        rel_path = img_file.relative_to(DOCUMENTS_PATH)
                        # Use POSIX path format for URL and normalize to NFC
                        url_path = unicodedata.normalize('NFC', rel_path.as_posix())
                        images.append({
                            'path': f'/documents/{url_path}',
                            'name': img_file.name
                        })
    except Exception as e:
        logging.warning(f"Error finding images: {e}")
    return images[:10]  # Limit to 10 images


@app.route('/api/stats')
def api_stats():
    """Get index statistics."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        # Format namespaces for frontend
        namespaces = []
        if stats.get('namespaces'):
            for ns_name, ns_info in stats['namespaces'].items():
                namespaces.append({
                    'name': ns_name if ns_name else '(ê¸°ë³¸)',
                    'vector_count': ns_info.vector_count
                })

        return jsonify({
            'success': True,
            'data': {
                'index_name': os.getenv("PINECONE_INDEX_NAME", "document-index"),
                'dimension': stats.get('dimension', 0),
                'total_vectors': stats.get('total_vector_count', 0),
                'namespaces': namespaces
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/search', methods=['POST'])
def api_search():
    """Search for similar content with optional hybrid/keyword modes."""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        try:
            top_k = max(1, min(int(data.get('top_k', 5)), 100))
        except (ValueError, TypeError):
            top_k = 5
        namespace = data.get('namespace', '')
        file_type = data.get('file_type', '')
        search_mode = data.get('search_mode', 'vector')  # vector | hybrid | keyword

        # 3-level metadata filters
        domain = data.get('domain', '')
        category = data.get('category', '')
        subcategory = data.get('subcategory', '')

        if not query:
            return jsonify({'success': False, 'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        agent = get_agent()

        # Build filter from file_type + metadata levels
        filter_dict = {}
        if file_type:
            filter_dict["file_type"] = file_type
        if domain:
            filter_dict["domain"] = domain
        if category:
            filter_dict["category"] = category
        if subcategory:
            filter_dict["subcategory"] = subcategory
        filter_dict = filter_dict or None

        # Always start with vector search to get candidate documents
        vector_results = agent.search(
            query=query,
            top_k=max(top_k, 20) if search_mode != 'vector' else top_k,
            namespace=namespace,
            filter=filter_dict
        )

        if search_mode == 'vector':
            # Pure vector search - existing behavior
            formatted_results = []
            for r in vector_results:
                metadata = r.get('metadata', {})
                formatted_results.append({
                    'score': round(r.get('score', 0), 4),
                    'source_file': metadata.get('source_file', 'N/A'),
                    'file_type': metadata.get('file_type', 'N/A'),
                    'content': metadata.get('content_preview', metadata.get('content', '')[:500]),
                    'filename': metadata.get('filename', ''),
                    'relative_path': metadata.get('relative_path', ''),
                    'ncs_category': metadata.get('ncs_category', ''),
                    'ncs_document_title': metadata.get('ncs_document_title', ''),
                    'ncs_section_type': metadata.get('ncs_section_type', ''),
                    'ncs_code': metadata.get('ncs_code', ''),
                })
        else:
            # hybrid or keyword mode - use HybridSearcher
            hybrid_searcher = get_hybrid_searcher_instance()
            hybrid_results = hybrid_searcher.search(
                query=query,
                vector_results=vector_results,
                top_k=top_k,
                build_index=True
            )

            if search_mode == 'keyword':
                # Re-sort by BM25 rank (keyword relevance)
                hybrid_results.sort(
                    key=lambda x: x.get('bm25_rank') or 9999
                )
                hybrid_results = hybrid_results[:top_k]

            formatted_results = []
            for r in hybrid_results:
                metadata = r.get('metadata', {})
                bm25_score_val = None
                if hasattr(hybrid_searcher, 'bm25_index') and hybrid_searcher.bm25_index:
                    # Calculate BM25 score for display
                    query_tokens = hybrid_searcher._tokenize(query)
                    scores = hybrid_searcher.bm25_index.get_scores(query_tokens)
                    content = metadata.get('content', '')
                    for idx, doc in hybrid_searcher.doc_map.items():
                        if doc.get('metadata', {}).get('content', '')[:200] == content[:200]:
                            bm25_score_val = round(scores[idx], 4)
                            break

                formatted_results.append({
                    'score': round(r.get('score', 0), 4),
                    'rrf_score': round(r.get('rrf_score', 0), 6),
                    'vector_rank': r.get('vector_rank'),
                    'bm25_rank': r.get('bm25_rank'),
                    'bm25_score': bm25_score_val,
                    'source_file': metadata.get('source_file', 'N/A'),
                    'file_type': metadata.get('file_type', 'N/A'),
                    'content': metadata.get('content_preview', metadata.get('content', '')[:500]),
                    'filename': metadata.get('filename', ''),
                    'relative_path': metadata.get('relative_path', ''),
                    'ncs_category': metadata.get('ncs_category', ''),
                    'ncs_document_title': metadata.get('ncs_document_title', ''),
                    'ncs_section_type': metadata.get('ncs_section_type', ''),
                    'ncs_code': metadata.get('ncs_code', ''),
                })

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'search_mode': search_mode,
                'count': len(formatted_results),
                'results': formatted_results
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


def _run_rag_pipeline(data):
    """Shared RAG search pipeline (Phase 1-7). Returns dict with context, sources, messages, tools, etc.
    Raises ValueError if query is empty. Returns early dict with 'answer' key if no results found."""
    query = data.get('query', '').strip()
    namespace = data.get('namespace', '')
    try:
        top_k = max(1, min(int(data.get('top_k', 10)), 100))
    except (ValueError, TypeError):
        top_k = 10
    use_enhancement = data.get('use_enhancement', True)

    logging.info(f"[API/ask] Query: {query[:50]}..., Namespace: {namespace}, TopK: {top_k}, Enhanced: {use_enhancement}")

    if not query:
        raise ValueError('ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.')

    # Parse @mentions for source filtering
    clean_query, mention_filters = parse_mentions(query)

    # Build search query
    if mention_filters:
        filter_keywords = ' '.join([f['value'].replace('_', ' ') for f in mention_filters])
        if clean_query and len(clean_query) >= 3:
            search_query = f"{filter_keywords} {clean_query}"
        else:
            search_query = filter_keywords
    else:
        search_query = clean_query if clean_query else query

    agent = get_agent()
    client = get_openai_client()

    # ========================================
    # Phase 1: Query Enhancement
    # ========================================
    enhanced_queries = [search_query]
    keywords = []

    if use_enhancement:
        try:
            query_enhancer = get_query_enhancer()

            # Generate query variations for broader retrieval
            enhanced_queries = query_enhancer.multi_query(search_query, num_variations=2)
            logging.info(f"[Query Enhancement] Generated {len(enhanced_queries)} query variations")

            # HyDE: generate hypothetical document for longer queries
            if len(search_query) >= 30:
                try:
                    domain = 'laborlaw' if namespace == 'laborlaw' else 'semiconductor'
                    hyde_doc = query_enhancer.hyde(search_query, domain=domain)
                    if hyde_doc and hyde_doc != search_query:
                        enhanced_queries.append(hyde_doc)
                        logging.info(f"[HyDE] Added hypothetical document query")
                except Exception as e:
                    logging.warning(f"[HyDE] Failed: {e}")

            # Extract keywords for potential filtering
            keywords = query_enhancer.extract_keywords(search_query)
            logging.info(f"[Query Enhancement] Keywords: {keywords}")

        except Exception as e:
            logging.warning(f"[Query Enhancement] Failed: {e}, using original query")
            enhanced_queries = [search_query]

    # ========================================
    # Phase 2: Multi-Query Search (with NCS metadata filtering)
    # ========================================
    # Build NCS-aware metadata filter from query
    ncs_filter = _build_ncs_filter(search_query)

    # Search with multiple query variations and merge results
    # When BM25 is skipped, fetch more candidates so the reranker has a wider pool
    skip_bm25_env = os.environ.get('SKIP_BM25_HYBRID', '').lower() in ('true', '1', 'yes')
    if mention_filters:
        search_top_k = top_k * 3
    elif skip_bm25_env:
        search_top_k = top_k * 4  # Wider recall when relying on reranker alone
    else:
        search_top_k = top_k * 2  # Fetch more for filtering/reranking
    is_all_namespace = (namespace == 'all')
    all_results = []
    seen_ids = set()

    for eq in enhanced_queries:
        try:
            if is_all_namespace:
                # Multi-namespace simultaneous query via Pinecone server-side parallelism
                try:
                    uploader = get_uploader()
                    stats = uploader.get_stats()
                    ns_list = [ns for ns in stats.get('namespaces', {}).keys() if ns]
                except Exception:
                    ns_list = ['semiconductor', 'laborlaw', 'field-training']
                results = agent.search_all_namespaces(
                    query=eq,
                    namespaces=ns_list,
                    top_k=search_top_k,
                    filter=ncs_filter
                )
            else:
                results = agent.search(
                    query=eq,
                    top_k=search_top_k,
                    namespace=namespace,
                    filter=ncs_filter
                )
            for r in results:
                # Deduplicate by content hash
                content = r.get('metadata', {}).get('content', '')
                content_id = hash(content[:200])
                if content_id not in seen_ids:
                    seen_ids.add(content_id)
                    all_results.append(r)
        except Exception as e:
            logging.warning(f"[Search] Failed for query '{eq[:30]}...': {e}")

    results = all_results
    logging.info(f"[Search] Retrieved {len(results)} unique documents from {len(enhanced_queries)} queries")

    # ========================================
    # Phase 3: Apply mention filters
    # ========================================
    if mention_filters and results:
        filtered_results = []
        for r in results:
            source_file = unicodedata.normalize('NFC', r.get('metadata', {}).get('source_file', ''))
            filename = unicodedata.normalize('NFC', r.get('metadata', {}).get('filename', ''))

            match = False
            for f in mention_filters:
                filter_value = unicodedata.normalize('NFC', f['value'].lower())
                if f['type'] == 'file':
                    if filter_value in filename.lower():
                        match = True
                        break
                elif f['type'] == 'folder':
                    if filter_value in source_file.lower():
                        match = True
                        break
                elif f['type'] == 'keyword':
                    if filter_value in source_file.lower():
                        match = True
                        break

            if match:
                filtered_results.append(r)

        results = filtered_results
        logging.info(f"[Filter] After mention filtering: {len(results)} documents")

    if not results:
        return {
            'early_response': True,
            'data': {
                'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                'sources': [],
                'enhancement_used': use_enhancement
            }
        }

    # ========================================
    # Phase 4: Hybrid Search (BM25 + Vector)
    # ========================================
    skip_bm25 = os.environ.get('SKIP_BM25_HYBRID', '').lower() in ('true', '1', 'yes')
    if skip_bm25:
        logging.info("[Hybrid Search] Skipped (SKIP_BM25_HYBRID=true)")
    elif use_enhancement and len(results) > 3:
        try:
            hybrid_searcher = get_hybrid_searcher_instance()
            results = hybrid_searcher.search_with_keyword_boost(
                query=search_query,
                vector_results=results,
                keywords=keywords,
                top_k=min(len(results), top_k * 2)
            )
            logging.info(f"[Hybrid Search] Applied BM25 + keyword boosting")
        except Exception as e:
            logging.warning(f"[Hybrid Search] Failed: {e}, using vector results only")

    # ========================================
    # Phase 5: Reranking
    # ========================================
    if use_enhancement and len(results) > 3:
        try:
            reranker = get_reranker_instance()
            results = reranker.rerank(
                query=search_query,
                docs=results,
                top_k=min(len(results), top_k + 5)  # Keep extra for context optimization
            )
            logging.info(f"[Reranking] Reranked to {len(results)} documents")
        except Exception as e:
            logging.warning(f"[Reranking] Failed: {e}, using original order")

    # ========================================
    # Phase 6: Context Optimization
    # ========================================
    if use_enhancement:
        try:
            context_optimizer = get_context_optimizer()

            # Deduplicate similar content
            results = context_optimizer.deduplicate(results)

            # Reorder for LLM attention (Lost in the Middle prevention)
            results = context_optimizer.reorder_for_llm(results, strategy="lost_in_middle")

            logging.info(f"[Context Optimization] Final context: {len(results)} documents")
        except Exception as e:
            logging.warning(f"[Context Optimization] Failed: {e}")

    # Filter by minimum relevance score
    MIN_RELEVANCE_SCORE = 0.3
    results = [r for r in results if r.get('rerank_score', r.get('rrf_score', r.get('score', 0))) >= MIN_RELEVANCE_SCORE]

    # Limit to top_k after all processing
    results = results[:top_k]

    # ========================================
    # Phase 7: Build Context and Sources
    # ========================================
    context_parts = []
    sources = []

    for i, r in enumerate(results):
        metadata = r.get('metadata', {})
        content = metadata.get('content', '')
        source_file = metadata.get('source_file', 'Unknown')
        file_type = metadata.get('file_type', 'unknown')
        score = r.get('rerank_score', r.get('rrf_score', r.get('score', 0)))

        if content:
            context_parts.append(f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {source_file})\n{content}")

            # Build image URL for image files (normalize unicode for macOS)
            image_url = None
            if file_type == 'image' or source_file.lower().endswith(('.jpeg', '.jpg', '.png', '.gif')):
                image_url = f'/documents/{unicodedata.normalize("NFC", source_file)}'

            sources.append({
                'source_file': source_file,
                'file_type': file_type,
                'score': round(score, 4) if isinstance(score, float) else score,
                'content_preview': content[:200] + '...' if len(content) > 200 else content,
                'image_url': image_url,
                'ncs_category': metadata.get('ncs_category', ''),
                'ncs_document_title': metadata.get('ncs_document_title', ''),
                'ncs_section_type': metadata.get('ncs_section_type', ''),
                'ncs_code': metadata.get('ncs_code', ''),
            })

    context = "\n\n---\n\n".join(context_parts)

    return {
        'early_response': False,
        'query': query,
        'namespace': namespace,
        'search_query': search_query,
        'context': context,
        'sources': sources,
        'context_parts': context_parts,
        'enhanced_queries': enhanced_queries,
        'keywords': keywords,
        'use_enhancement': use_enhancement,
        'client': client,
    }


@app.route('/api/ask', methods=['POST'])
def api_ask():
    """RAG endpoint - search and generate comprehensive answer with enhanced retrieval."""
    try:
        data = request.get_json()
        pipeline = _run_rag_pipeline(data)

        if pipeline.get('early_response'):
            return jsonify({'success': True, 'data': pipeline['data']})

        query = pipeline['query']
        namespace = pipeline['namespace']
        context = pipeline['context']
        sources = pipeline['sources']
        enhanced_queries = pipeline['enhanced_queries']
        keywords = pipeline['keywords']
        use_enhancement = pipeline['use_enhancement']
        client = pipeline['client']

        # ========================================
        # Phase 8: Generate Answer with Improved Prompt
        # ========================================
        # Select domain-specific prompt based on namespace
        base_prompt = DOMAIN_PROMPTS.get(namespace, DEFAULT_SYSTEM_PROMPT)

        # Enhanced Chain-of-Thought prompt additions
        cot_instructions = """

## ë‹µë³€ ìƒì„± í”„ë¡œì„¸ìŠ¤

1. **ì´í•´ ë‹¨ê³„**: ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. **ë¶„ì„ ë‹¨ê³„**: ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
3. **ì¢…í•© ë‹¨ê³„**: ì¶”ì¶œëœ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”
4. **ê²€ì¦ ë‹¨ê³„**: ë‹µë³€ì´ ì§ˆë¬¸ì— ì™„ì „íˆ ë‹µí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”

## ë‹µë³€ êµ¬ì¡°í™” ì§€ì¹¨

- **í•µì‹¬ ìš”ì•½**: ë¨¼ì € 1-2ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‹µë³€ ì œì‹œ
- **ìƒì„¸ ì„¤ëª…**: ê´€ë ¨ ê°œë…, ì›ë¦¬, í”„ë¡œì„¸ìŠ¤ ì„¤ëª…
- **êµ¬ì²´ì  ì˜ˆì‹œ**: ê°€ëŠ¥í•œ ê²½ìš° ì‹¤ì œ ì‚¬ë¡€ë‚˜ ìˆ˜ì¹˜ ì œì‹œ
- **ì¶”ê°€ ì •ë³´**: ê´€ë ¨ëœ ë³´ì¶© ì •ë³´ë‚˜ ì£¼ì˜ì‚¬í•­"""

        # Common visual representation guidelines (appended to all prompts)
        visual_guidelines = """

## ì‹œê°ì  í‘œí˜„ ì§€ì¹¨

10. **ë¹„êµ ì •ë³´**ëŠ” ë°˜ë“œì‹œ í‘œ(table)ë¡œ ì •ë¦¬í•˜ì„¸ìš”
   ì˜ˆì‹œ:
   | êµ¬ë¶„ | í•­ëª©1 | í•­ëª©2 |
   |------|-------|-------|
   | íŠ¹ì§• | ì„¤ëª…1 | ì„¤ëª…2 |

11. **í”„ë¡œì„¸ìŠ¤/ë‹¨ê³„**ëŠ” ìˆœì„œ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
12. **ìˆ˜ì¹˜ ë°ì´í„°**ê°€ ìˆìœ¼ë©´ ì •ë¦¬í•´ì„œ ì œì‹œí•˜ì„¸ìš” (ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆë„ë¡)
13. **ë¶„ë¥˜/ì¢…ë¥˜**ëŠ” í‘œë‚˜ ëª©ë¡ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”

ë°ì´í„° ì‹œê°í™” ìš”ì²­ì‹œ:
- ìˆ˜ì¹˜ ë¹„êµê°€ í•„ìš”í•œ ê²½ìš°, ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ì œê³µí•˜ì„¸ìš”:
<!--CHART_DATA
{
  "type": "bar|pie|line",
  "title": "ì°¨íŠ¸ ì œëª©",
  "labels": ["ë¼ë²¨1", "ë¼ë²¨2"],
  "data": [ìˆ˜ì¹˜1, ìˆ˜ì¹˜2],
  "unit": "ë‹¨ìœ„"
}
CHART_DATA-->"""

        system_prompt = base_prompt + cot_instructions + visual_guidelines

        user_prompt = f"""## ì§ˆë¬¸
{query}

## ì°¸ê³  ë¬¸ì„œ ({len(sources)}ê°œ)
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨:**
1. ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ [1], [2] ë“±ì˜ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œí•˜ê³ , ìƒì„¸ ì„¤ëª…ì„ ì´ì–´ê°€ì„¸ìš”
4. ê¸°ìˆ  ìš©ì–´ëŠ” í•œê¸€ê³¼ ì˜ë¬¸ì„ ë³‘ê¸°í•˜ì„¸ìš”"""

        # Enable calculator functions only for laborlaw namespace
        tools = CALCULATOR_FUNCTIONS if namespace == 'laborlaw' else None

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # Initial GPT call (with function calling if tools available)
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=tools,
            tool_choice="auto" if tools else None,
            temperature=0.3,
            max_tokens=2500  # Increased for more detailed responses
        )

        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # Handle function calls
        calculation_results = []
        if tool_calls:
            messages.append(response_message)

            for tool_call in tool_calls:
                function_name = tool_call.function.name

                # Parse function arguments with error handling
                try:
                    function_args = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError as e:
                    logging.error(f"[Function Call] Invalid JSON args for {function_name}: {e}")
                    logging.error(f"[Function Call] Raw arguments: {tool_call.function.arguments}")
                    function_response = {"error": f"ì˜ëª»ëœ í•¨ìˆ˜ ì¸ì í˜•ì‹: {str(e)}"}
                    calculation_results.append({
                        'function': function_name,
                        'args': {},
                        'result': function_response
                    })
                    # Add error response to messages
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": json.dumps(function_response, ensure_ascii=False, indent=2)
                    })
                    continue

                logging.info(f"[Function Call] {function_name} with args: {function_args}")

                # Execute the function
                try:
                    if function_name == "calculate_wage":
                        function_response = calculate_wage(**function_args)
                    elif function_name == "calculate_insurance":
                        function_response = calculate_insurance(**function_args)
                    else:
                        function_response = {"error": "Unknown function"}
                except Exception as e:
                    logging.error(f"[Function Call] Execution error in {function_name}: {e}")
                    function_response = {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}

                calculation_results.append({
                    'function': function_name,
                    'args': function_args,
                    'result': function_response
                })

                # Add function response to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": json.dumps(function_response, ensure_ascii=False, indent=2)
                })

            # Get final response with function results
            second_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.3,
                max_tokens=2500
            )

            answer = second_response.choices[0].message.content
        else:
            answer = response_message.content

        # Collect related images from source documents
        related_images = []
        seen_folders = set()
        for source in sources:
            source_file = source.get('source_file', '')
            # Extract folder path to avoid duplicates
            folder_key = '/'.join(source_file.split('/')[:3])
            if folder_key not in seen_folders:
                seen_folders.add(folder_key)
                images = find_related_images(source_file)
                related_images.extend(images)

        # Limit total images
        related_images = related_images[:12]

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'answer': answer,
                'sources': sources,
                'source_count': len(sources),
                'images': related_images,
                'calculations': calculation_results if calculation_results else None,
                'enhancement_used': use_enhancement,
                'query_variations': enhanced_queries if use_enhancement else None,
                'keywords_extracted': keywords if use_enhancement else None
            }
        })

    except Exception as e:
        logging.exception(f"[API/ask] Error: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


def _build_llm_messages(query, sources, context, namespace):
    """Build system prompt and user message for LLM call."""
    base_prompt = DOMAIN_PROMPTS.get(namespace, DEFAULT_SYSTEM_PROMPT)

    cot_instructions = """

## ë‹µë³€ ìƒì„± í”„ë¡œì„¸ìŠ¤

1. **ì´í•´ ë‹¨ê³„**: ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. **ë¶„ì„ ë‹¨ê³„**: ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
3. **ì¢…í•© ë‹¨ê³„**: ì¶”ì¶œëœ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”
4. **ê²€ì¦ ë‹¨ê³„**: ë‹µë³€ì´ ì§ˆë¬¸ì— ì™„ì „íˆ ë‹µí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”

## ë‹µë³€ êµ¬ì¡°í™” ì§€ì¹¨

- **í•µì‹¬ ìš”ì•½**: ë¨¼ì € 1-2ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‹µë³€ ì œì‹œ
- **ìƒì„¸ ì„¤ëª…**: ê´€ë ¨ ê°œë…, ì›ë¦¬, í”„ë¡œì„¸ìŠ¤ ì„¤ëª…
- **êµ¬ì²´ì  ì˜ˆì‹œ**: ê°€ëŠ¥í•œ ê²½ìš° ì‹¤ì œ ì‚¬ë¡€ë‚˜ ìˆ˜ì¹˜ ì œì‹œ
- **ì¶”ê°€ ì •ë³´**: ê´€ë ¨ëœ ë³´ì¶© ì •ë³´ë‚˜ ì£¼ì˜ì‚¬í•­"""

    visual_guidelines = """

## ì‹œê°ì  í‘œí˜„ ì§€ì¹¨

10. **ë¹„êµ ì •ë³´**ëŠ” ë°˜ë“œì‹œ í‘œ(table)ë¡œ ì •ë¦¬í•˜ì„¸ìš”
   ì˜ˆì‹œ:
   | êµ¬ë¶„ | í•­ëª©1 | í•­ëª©2 |
   |------|-------|-------|
   | íŠ¹ì§• | ì„¤ëª…1 | ì„¤ëª…2 |

11. **í”„ë¡œì„¸ìŠ¤/ë‹¨ê³„**ëŠ” ìˆœì„œ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
12. **ìˆ˜ì¹˜ ë°ì´í„°**ê°€ ìˆìœ¼ë©´ ì •ë¦¬í•´ì„œ ì œì‹œí•˜ì„¸ìš” (ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆë„ë¡)
13. **ë¶„ë¥˜/ì¢…ë¥˜**ëŠ” í‘œë‚˜ ëª©ë¡ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”

ë°ì´í„° ì‹œê°í™” ìš”ì²­ì‹œ:
- ìˆ˜ì¹˜ ë¹„êµê°€ í•„ìš”í•œ ê²½ìš°, ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ì œê³µí•˜ì„¸ìš”:
<!--CHART_DATA
{
  "type": "bar|pie|line",
  "title": "ì°¨íŠ¸ ì œëª©",
  "labels": ["ë¼ë²¨1", "ë¼ë²¨2"],
  "data": [ìˆ˜ì¹˜1, ìˆ˜ì¹˜2],
  "unit": "ë‹¨ìœ„"
}
CHART_DATA-->"""

    system_prompt = base_prompt + cot_instructions + visual_guidelines

    user_prompt = f"""## ì§ˆë¬¸
{query}

## ì°¸ê³  ë¬¸ì„œ ({len(sources)}ê°œ)
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨:**
1. ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ [1], [2] ë“±ì˜ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œí•˜ê³ , ìƒì„¸ ì„¤ëª…ì„ ì´ì–´ê°€ì„¸ìš”
4. ê¸°ìˆ  ìš©ì–´ëŠ” í•œê¸€ê³¼ ì˜ë¬¸ì„ ë³‘ê¸°í•˜ì„¸ìš”"""

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]


def _handle_tool_calls(client, messages, tool_calls):
    """Execute tool calls and return (calculation_results, updated_messages)."""
    calculation_results = []
    messages.append(tool_calls)  # append the assistant message with tool_calls

    for tool_call in tool_calls.tool_calls:
        function_name = tool_call.function.name
        try:
            function_args = json.loads(tool_call.function.arguments)
        except json.JSONDecodeError as e:
            logging.error(f"[Function Call] Invalid JSON args for {function_name}: {e}")
            function_response = {"error": f"ì˜ëª»ëœ í•¨ìˆ˜ ì¸ì í˜•ì‹: {str(e)}"}
            calculation_results.append({'function': function_name, 'args': {}, 'result': function_response})
            messages.append({
                "role": "tool", "tool_call_id": tool_call.id,
                "name": function_name,
                "content": json.dumps(function_response, ensure_ascii=False, indent=2)
            })
            continue

        logging.info(f"[Function Call] {function_name} with args: {function_args}")
        try:
            if function_name == "calculate_wage":
                function_response = calculate_wage(**function_args)
            elif function_name == "calculate_insurance":
                function_response = calculate_insurance(**function_args)
            else:
                function_response = {"error": "Unknown function"}
        except Exception as e:
            logging.error(f"[Function Call] Execution error in {function_name}: {e}")
            function_response = {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}

        calculation_results.append({'function': function_name, 'args': function_args, 'result': function_response})
        messages.append({
            "role": "tool", "tool_call_id": tool_call.id,
            "name": function_name,
            "content": json.dumps(function_response, ensure_ascii=False, indent=2)
        })

    return calculation_results, messages


@app.route('/api/ask/stream', methods=['POST'])
def api_ask_stream():
    """SSE streaming RAG endpoint - sends metadata first, then streams LLM answer token by token."""
    try:
        data = request.get_json()
        pipeline = _run_rag_pipeline(data)
    except ValueError as e:
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.exception(f"[API/ask/stream] Pipeline error: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})

    if pipeline.get('early_response'):
        return jsonify({'success': True, 'data': pipeline['data']})

    query = pipeline['query']
    namespace = pipeline['namespace']
    context = pipeline['context']
    sources = pipeline['sources']
    enhanced_queries = pipeline['enhanced_queries']
    keywords = pipeline['keywords']
    use_enhancement = pipeline['use_enhancement']
    client = pipeline['client']

    messages = _build_llm_messages(query, sources, context, namespace)
    tools = CALCULATOR_FUNCTIONS if namespace == 'laborlaw' else None

    def generate():
        try:
            calculation_results = []

            # Collect related images
            related_images = []
            seen_folders = set()
            for source in sources:
                source_file = source.get('source_file', '')
                folder_key = '/'.join(source_file.split('/')[:3])
                if folder_key not in seen_folders:
                    seen_folders.add(folder_key)
                    images = find_related_images(source_file)
                    related_images.extend(images)
            related_images = related_images[:12]

            # Send metadata event first (sources, images, query info)
            metadata_event = json.dumps({
                'type': 'metadata',
                'data': {
                    'query': query,
                    'sources': sources,
                    'source_count': len(sources),
                    'images': related_images,
                    'enhancement_used': use_enhancement,
                    'query_variations': enhanced_queries if use_enhancement else None,
                    'keywords_extracted': keywords if use_enhancement else None
                }
            }, ensure_ascii=False)
            yield f"data: {metadata_event}\n\n"

            # If tools available, first call is non-streaming to check for tool_calls
            if tools:
                first_response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    temperature=0.3,
                    max_tokens=2500
                )
                response_message = first_response.choices[0].message

                if response_message.tool_calls:
                    calc_results, messages_updated = _handle_tool_calls(client, messages, response_message)
                    calculation_results = calc_results

                    # Send calculation results event
                    calc_event = json.dumps({
                        'type': 'calculations',
                        'data': calculation_results
                    }, ensure_ascii=False)
                    yield f"data: {calc_event}\n\n"

                    # Stream the final response after tool calls
                    stream = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=messages_updated,
                        temperature=0.3,
                        max_tokens=2500,
                        stream=True
                    )
                    for chunk in stream:
                        if chunk.choices and chunk.choices[0].delta.content:
                            token = chunk.choices[0].delta.content
                            token_event = json.dumps({'type': 'token', 'data': token}, ensure_ascii=False)
                            yield f"data: {token_event}\n\n"
                else:
                    # No tool calls - stream the content from first response
                    if response_message.content:
                        # First response was non-streaming, send it as tokens
                        token_event = json.dumps({'type': 'token', 'data': response_message.content}, ensure_ascii=False)
                        yield f"data: {token_event}\n\n"
            else:
                # No tools - stream directly
                stream = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2500,
                    stream=True
                )
                for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        token = chunk.choices[0].delta.content
                        token_event = json.dumps({'type': 'token', 'data': token}, ensure_ascii=False)
                        yield f"data: {token_event}\n\n"

            # Send done event
            done_event = json.dumps({
                'type': 'done',
                'data': {'calculations': calculation_results if calculation_results else None}
            }, ensure_ascii=False)
            yield f"data: {done_event}\n\n"

        except Exception as e:
            logging.exception(f"[API/ask/stream] Streaming error: {str(e)}")
            error_event = json.dumps({'type': 'error', 'data': str(e)}, ensure_ascii=False)
            yield f"data: {error_event}\n\n"

    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )


@app.route('/api/namespaces')
def api_namespaces():
    """Get list of namespaces."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        namespaces = []
        if stats.get('namespaces'):
            for ns_name in stats['namespaces'].keys():
                namespaces.append(ns_name if ns_name else '')

        return jsonify({
            'success': True,
            'data': namespaces
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/sources')
def api_sources():
    """Get list of available source files and folders for autocomplete."""
    try:
        namespace = request.args.get('namespace', '')
        agent = get_agent()

        # Use generic queries based on namespace to find documents
        # Different queries for different document domains
        generic_queries = ["ë¬¸ì„œ", "ì •ë³´", "ë‚´ìš©", "ë²•ë¥ ", "ê·œì •", "ê¸°ìˆ "]

        all_results = []
        for query in generic_queries[:2]:  # Use top 2 queries to reduce API calls
            results = agent.search(
                query=query,
                top_k=50,
                namespace=namespace
            )
            all_results.extend(results)

        # Deduplicate by source_file
        seen = set()
        results = []
        for r in all_results:
            source_file = r.get('metadata', {}).get('source_file', '')
            if source_file and source_file not in seen:
                seen.add(source_file)
                results.append(r)

        folders = set()
        files = set()

        for r in results:
            metadata = r.get('metadata', {})
            source_file = metadata.get('source_file', '')
            filename = metadata.get('filename', '')

            if filename:
                files.add(filename)

            if source_file:
                # Extract folder paths
                parts = source_file.split('/')
                for i in range(1, len(parts)):
                    folder = '/'.join(parts[:i])
                    if folder:
                        folders.add(folder)

        return jsonify({
            'success': True,
            'data': {
                'folders': sorted(folders),
                'files': sorted(files)
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/delete', methods=['POST'])
def api_delete():
    """Delete vectors."""
    try:
        data = request.get_json()
        namespace = data.get('namespace', '')
        delete_all = data.get('delete_all', False)
        source_file = data.get('source_file', '')

        uploader = get_uploader()

        if delete_all:
            uploader.index.delete(delete_all=True, namespace=namespace)
            return jsonify({
                'success': True,
                'message': f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace or '(ê¸°ë³¸)'}' ì˜ ëª¨ë“  ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            })
        elif source_file:
            success = uploader.delete_by_filter(
                filter={"source_file": source_file},
                namespace=namespace
            )
            if success:
                return jsonify({
                    'success': True,
                    'message': f"'{source_file}'ì˜ ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({'success': False, 'error': 'ì‚­ì œ ì‹¤íŒ¨'})
        else:
            return jsonify({'success': False, 'error': 'ì‚­ì œí•  ëŒ€ìƒì„ ì§€ì •í•´ì£¼ì„¸ìš”.'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


# MSDS API Client instance
msds_client = MsdsApiClient()


@app.route('/api/msds/search', methods=['POST'])
def msds_search():
    """Search chemicals in KOSHA MSDS database."""
    try:
        data = request.json
        search_word = data.get('search_word', '')
        search_type = int(data.get('search_type', 0))  # 0=êµ­ë¬¸ëª…, 1=CAS, 2=UN, 3=KE, 4=EN
        page_no = int(data.get('page_no', 1))
        num_of_rows = int(data.get('num_of_rows', 10))

        if not search_word:
            return jsonify({'success': False, 'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        result = msds_client.search_chemicals(
            search_word=search_word,
            search_type=search_type,
            page_no=page_no,
            num_of_rows=num_of_rows
        )

        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/msds/detail', methods=['POST'])
def msds_detail():
    """Get detailed chemical information from KOSHA MSDS."""
    try:
        data = request.json
        chem_id = data.get('chem_id', '')
        section = data.get('section', '')  # Optional: specific section (01-16)

        if not chem_id:
            return jsonify({'success': False, 'error': 'í™”í•™ë¬¼ì§ˆ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        # If section is specified, get that section only
        if section:
            result = msds_client.get_chemical_detail(chem_id, section)
        else:
            # Get all sections
            result = msds_client.get_full_chemical_detail(chem_id)

        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/msds/identify', methods=['POST'])
def msds_identify():
    """Identify chemical substance from image using OpenAI Vision API."""
    try:
        data = request.json
        image_data = data.get('image', '')  # Base64 encoded image

        if not image_data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'})

        # Remove data URL prefix if present
        if ',' in image_data:
            image_data = image_data.split(',')[1]

        # Use OpenAI Vision API to identify chemical
        client = get_openai_client()

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """ì´ë¯¸ì§€ì—ì„œ í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
1. í™”í•™ë¬¼ì§ˆëª… (êµ­ë¬¸ëª… ë˜ëŠ” ì˜ë¬¸ëª…)
2. CAS ë²ˆí˜¸ (ìˆëŠ” ê²½ìš°)
3. ì œí’ˆëª… ë˜ëŠ” ìƒí’ˆëª… (ìˆëŠ” ê²½ìš°)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”:
{
    "chemical_name": "í™”í•™ë¬¼ì§ˆëª…",
    "cas_no": "CAS ë²ˆí˜¸ (ì—†ìœ¼ë©´ null)",
    "product_name": "ì œí’ˆëª… (ì—†ìœ¼ë©´ null)"
}

ì´ë¯¸ì§€ì—ì„œ í™”í•™ë¬¼ì§ˆ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´:
{
    "chemical_name": null,
    "cas_no": null,
    "product_name": null,
    "error": "í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        # Parse response
        result_text = response.choices[0].message.content.strip()

        # Extract JSON from response
        # Try to find JSON in the response
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            # If no JSON found, try to parse the whole response
            result = json.loads(result_text)

        # If chemical name found, search for it
        if result.get('chemical_name'):
            search_word = result['chemical_name']
            search_type = 0  # Default to Korean name

            # If CAS number is available, use that for search
            if result.get('cas_no'):
                search_word = result['cas_no']
                search_type = 1  # CAS number search

            # Search in MSDS database
            search_result = msds_client.search_chemicals(
                search_word=search_word,
                search_type=search_type,
                page_no=1,
                num_of_rows=10
            )

            return jsonify({
                'success': True,
                'identified': result,
                'search_result': search_result
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'),
                'identified': result
            })

    except json.JSONDecodeError as e:
        return jsonify({
            'success': False,
            'error': f'ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}',
            'raw_response': result_text if 'result_text' in locals() else None
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


if __name__ == '__main__':
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("Error: OPENAI_API_KEY not set")
        exit(1)
    if not os.getenv("PINECONE_API_KEY"):
        print("Error: PINECONE_API_KEY not set")
        exit(1)

    print("ğŸš€ Pinecone Agent Web Interface")
    print("=" * 40)
    print(f"Index: {os.getenv('PINECONE_INDEX_NAME', 'document-index')}")
    print("=" * 40)
    print("\nğŸŒ http://localhost:5001 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”\n")

    app.run(debug=True, host='0.0.0.0', port=5001)
