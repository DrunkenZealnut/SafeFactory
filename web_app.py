#!/usr/bin/env python3
"""
Pinecone Agent Web Interface
Flask-based web UI for Pinecone vector database operations with RAG support.
"""

import os
import re
import json
import unicodedata
from pathlib import Path
from flask import Flask, render_template, request, jsonify, Response, stream_with_context
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = os.urandom(24)

# Global instances (lazy initialization)
_agent = None
_openai_client = None

def get_openai_client():
    """Get or create OpenAI client."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    return _openai_client

def get_agent():
    """Get or create the PineconeAgent instance."""
    global _agent
    if _agent is None:
        from src.agent import PineconeAgent
        _agent = PineconeAgent(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            pinecone_api_key=os.getenv("PINECONE_API_KEY"),
            pinecone_index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
            create_index_if_not_exists=False
        )
    return _agent

def get_uploader():
    """Get PineconeUploader for stats."""
    from src.pinecone_uploader import PineconeUploader
    return PineconeUploader(
        api_key=os.getenv("PINECONE_API_KEY"),
        index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
        create_if_not_exists=False
    )


def parse_mentions(query):
    """Parse @mentions from query and return (clean_query, filters).

    Supports:
    - @íŒŒì¼ëª….md - specific file
    - @í´ë”ëª…/ - folder path (ends with /)
    - @í‚¤ì›Œë“œ - partial match on source_file
    """
    mentions = re.findall(r'@([^\s@]+)', query)
    clean_query = re.sub(r'@[^\s@]+', '', query).strip()

    filters = []
    for mention in mentions:
        if mention.endswith('/'):
            # Folder filter
            filters.append({'type': 'folder', 'value': mention.rstrip('/')})
        elif '.' in mention:
            # File filter (has extension)
            filters.append({'type': 'file', 'value': mention})
        else:
            # Keyword filter
            filters.append({'type': 'keyword', 'value': mention})

    return clean_query, filters


def build_source_filter(filters):
    """Build Pinecone filter from parsed mentions.

    Note: Pinecone doesn't support substring matching directly,
    so we'll filter results post-query.
    """
    # For now, return None and do post-filtering
    # Pinecone filter would require exact match or $in operator
    return None


@app.route('/')
def index():
    """Main dashboard page."""
    return render_template('index.html')


@app.route('/api/stats')
def api_stats():
    """Get index statistics."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        # Format namespaces for frontend
        namespaces = []
        if stats.get('namespaces'):
            for ns_name, ns_info in stats['namespaces'].items():
                namespaces.append({
                    'name': ns_name if ns_name else '(ê¸°ë³¸)',
                    'vector_count': ns_info.vector_count
                })

        return jsonify({
            'success': True,
            'data': {
                'index_name': os.getenv("PINECONE_INDEX_NAME", "document-index"),
                'dimension': stats.get('dimension', 0),
                'total_vectors': stats.get('total_vector_count', 0),
                'namespaces': namespaces
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/search', methods=['POST'])
def api_search():
    """Search for similar content."""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        top_k = int(data.get('top_k', 5))
        namespace = data.get('namespace', '')
        file_type = data.get('file_type', '')

        if not query:
            return jsonify({'success': False, 'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        agent = get_agent()

        # Build filter
        filter_dict = None
        if file_type:
            filter_dict = {"file_type": file_type}

        results = agent.search(
            query=query,
            top_k=top_k,
            namespace=namespace,
            filter=filter_dict
        )

        # Format results for frontend
        formatted_results = []
        for r in results:
            metadata = r.get('metadata', {})
            formatted_results.append({
                'score': round(r.get('score', 0), 4),
                'source_file': metadata.get('source_file', 'N/A'),
                'file_type': metadata.get('file_type', 'N/A'),
                'content': metadata.get('content', '')[:500],
                'filename': metadata.get('filename', ''),
                'relative_path': metadata.get('relative_path', '')
            })

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'count': len(formatted_results),
                'results': formatted_results
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/ask', methods=['POST'])
def api_ask():
    """RAG endpoint - search and generate comprehensive answer."""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        namespace = data.get('namespace', '')
        top_k = int(data.get('top_k', 10))  # More documents for better context

        if not query:
            return jsonify({'success': False, 'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        # Parse @mentions for source filtering
        clean_query, mention_filters = parse_mentions(query)

        # Build search query: include filter keywords to improve relevance
        if mention_filters:
            filter_keywords = ' '.join([f['value'].replace('_', ' ') for f in mention_filters])
            if clean_query and len(clean_query) >= 3:
                search_query = f"{filter_keywords} {clean_query}"
            else:
                search_query = filter_keywords
        else:
            search_query = clean_query if clean_query else query

        agent = get_agent()
        client = get_openai_client()

        # Step 1: Search for relevant documents (fetch more if filtering)
        # When filtering, we need to fetch significantly more results
        search_top_k = top_k * 5 if mention_filters else top_k
        results = agent.search(
            query=search_query,
            top_k=search_top_k,
            namespace=namespace
        )

        # Step 1.5: Apply mention filters (post-query filtering)
        # Note: Use Unicode NFC normalization to handle Korean character encoding differences
        if mention_filters and results:
            filtered_results = []
            for r in results:
                source_file = unicodedata.normalize('NFC', r.get('metadata', {}).get('source_file', ''))
                filename = unicodedata.normalize('NFC', r.get('metadata', {}).get('filename', ''))

                match = False
                for f in mention_filters:
                    filter_value = unicodedata.normalize('NFC', f['value'].lower())
                    if f['type'] == 'file':
                        if filter_value in filename.lower():
                            match = True
                            break
                    elif f['type'] == 'folder':
                        if filter_value in source_file.lower():
                            match = True
                            break
                    elif f['type'] == 'keyword':
                        if filter_value in source_file.lower():
                            match = True
                            break

                if match:
                    filtered_results.append(r)

            results = filtered_results[:top_k]

        if not results:
            return jsonify({
                'success': True,
                'data': {
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                    'sources': []
                }
            })

        # Step 2: Build context from search results
        context_parts = []
        sources = []

        for i, r in enumerate(results):
            metadata = r.get('metadata', {})
            content = metadata.get('content', '')
            source_file = metadata.get('source_file', 'Unknown')
            file_type = metadata.get('file_type', 'unknown')
            score = r.get('score', 0)

            if content:
                context_parts.append(f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {source_file})\n{content}")
                sources.append({
                    'source_file': source_file,
                    'file_type': file_type,
                    'score': round(score, 4),
                    'content_preview': content[:200] + '...' if len(content) > 200 else content
                })

        context = "\n\n---\n\n".join(context_parts)

        # Step 3: Generate comprehensive answer using GPT
        system_prompt = """ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **ì¤‘ìš”**: ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. ì˜ˆ: "CVD ê³µì •ì€ í™”í•™ ê¸°ìƒ ì¦ì°© ë°©ì‹ì…ë‹ˆë‹¤[1]."
3. ì¸ìš© í˜•ì‹: ë¬¸ì¥ ëì— [1], [2] ë“±ì˜ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì–´ë–¤ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
4. ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•  ë•ŒëŠ” [1][3]ì²˜ëŸ¼ ë³µìˆ˜ ì¸ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤
5. ê¸°ìˆ  ìš©ì–´ëŠ” í•œê¸€ê³¼ ì˜ë¬¸ì„ ë³‘ê¸°í•˜ì„¸ìš”
6. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì„¸ìš”
7. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
8. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”"""

        user_prompt = f"""## ì§ˆë¬¸
{query}

## ì°¸ê³  ë¬¸ì„œ
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
**ë°˜ë“œì‹œ ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ [1], [2] ë“±ì˜ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”.**"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        answer = response.choices[0].message.content

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'answer': answer,
                'sources': sources,
                'source_count': len(sources)
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/namespaces')
def api_namespaces():
    """Get list of namespaces."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        namespaces = []
        if stats.get('namespaces'):
            for ns_name in stats['namespaces'].keys():
                namespaces.append(ns_name if ns_name else '')

        return jsonify({
            'success': True,
            'data': namespaces
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/sources')
def api_sources():
    """Get list of available source files and folders for autocomplete."""
    try:
        namespace = request.args.get('namespace', '')
        agent = get_agent()

        # Search with a generic query to get sample of documents
        results = agent.search(
            query="ë°˜ë„ì²´ ê³µì • ê¸°ìˆ ",
            top_k=100,
            namespace=namespace
        )

        folders = set()
        files = set()

        for r in results:
            metadata = r.get('metadata', {})
            source_file = metadata.get('source_file', '')
            filename = metadata.get('filename', '')

            if filename:
                files.add(filename)

            if source_file:
                # Extract folder paths
                parts = source_file.split('/')
                for i in range(1, len(parts)):
                    folder = '/'.join(parts[:i])
                    if folder:
                        folders.add(folder)

        return jsonify({
            'success': True,
            'data': {
                'folders': sorted(folders),
                'files': sorted(files)
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/delete', methods=['POST'])
def api_delete():
    """Delete vectors."""
    try:
        data = request.get_json()
        namespace = data.get('namespace', '')
        delete_all = data.get('delete_all', False)
        source_file = data.get('source_file', '')

        uploader = get_uploader()

        if delete_all:
            uploader.index.delete(delete_all=True, namespace=namespace)
            return jsonify({
                'success': True,
                'message': f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace or '(ê¸°ë³¸)'}' ì˜ ëª¨ë“  ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            })
        elif source_file:
            success = uploader.delete_by_filter(
                filter={"source_file": source_file},
                namespace=namespace
            )
            if success:
                return jsonify({
                    'success': True,
                    'message': f"'{source_file}'ì˜ ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({'success': False, 'error': 'ì‚­ì œ ì‹¤íŒ¨'})
        else:
            return jsonify({'success': False, 'error': 'ì‚­ì œí•  ëŒ€ìƒì„ ì§€ì •í•´ì£¼ì„¸ìš”.'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


if __name__ == '__main__':
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("Error: OPENAI_API_KEY not set")
        exit(1)
    if not os.getenv("PINECONE_API_KEY"):
        print("Error: PINECONE_API_KEY not set")
        exit(1)

    print("ğŸš€ Pinecone Agent Web Interface")
    print("=" * 40)
    print(f"Index: {os.getenv('PINECONE_INDEX_NAME', 'document-index')}")
    print("=" * 40)
    print("\nğŸŒ http://localhost:5001 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”\n")

    app.run(debug=True, host='0.0.0.0', port=5001)
